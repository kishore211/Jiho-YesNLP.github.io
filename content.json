{"posts":[{"title":"Language Models, BERT to ChatGPT","text":"I am hearing ‘ChatGPT’ from everywhere. Probably, you do too. I hadn’t looked into it until recently; because I thought this was another language model trained with bunch of data. (Actually, I have hundreds of paper “to read” and I didn’t want to add another one to my pile.) So, eventually I played with it, and yes I was shocked by the quality of the generated responses. Here are some examples. You can even ask it to program~! And it will return a working code even with comments. (Well, now we have GPT-4 which you can use to code a website out of a mockup image of your website.) I believe now the academia (at least) is in real danger. It seems like for now we do not have a way to tell whether a writing is created by a human or generated by a pre-trained language model (PLM). I ran a plagiarism checker and saw if this code was a copy of any piece of code accessible on the Internet. I used Turnitin and the plagiarism score was only 9% (which is much less than a homework submission of an average college class). A possible source of the above code is Qiskit, which has a code base for open-source quantum development (Qiskit.optimization.applications.ising.knapsack), and I don’t think it’s likely that the code is a copy. Now, already there are a few scientific articles that added GPT as a co-author of the paper, which are accepted as legitimate papers. Again, GPT is another transformer-based language model; but, trained really well. However, there’s no way I can just close my eyes and treat it as another PLM. And, anyone who’s studying machine learning should understand how it become a thing. What is (Chat)GPT? According to Wikipedia, ChatGPT is… Generative Pretrained Transformer (GPT), Based on OpenAI’s GPT-3 family of Large Language Model (LLM), Fine-tuned using both supervised and reinforcement learning.Fine-tuned using both supervised and reinforcement learning. Alright, so let’s start with Transformer Attention Mechanism and TransformerIn order to understand Transformer, we need to go back all the way to Recurrent Neural Network (RNN) and Language Model (LM) Recurrent Neural NetworkRNN is a type of neural network designed for processing sequential data, such as temporal or linguistic data. Unlike feedforward networks, RNNs maintain a hidden state that passes the previous information over time steps. At each time step, the hidden state is updated based on the current input and the previous hidden state. This allows the model to capture temporal dependencies in the data. We can mathematically describe the forward pass of RNN as below: ht=tanh(Wht−1+Uxt+b)yt=softmax(Vht+c)h_t = tanh(Wh_{t-1} + Ux_t + b) y_t = softmax(Vh_t + c) where: x_t is the current input vector h_t is the hidden state vector U, W, and V are the learnable weight parameters for the input-to-hidden, hidden-to-hidden, hidden-to-output, respectively b and c are the bias vectors for hidden and output layers, respectively RNNs are a popular choice for the neural machine translation (NMT) models. The basic architecture of an NMT model consists of an encoder RNN and decoder RNN. The encoder RNN takes the input sentence in the source language and it generates a fixed-length vector representation (context vector) of the entire sentence at the end. The decoder RNN takes the context vector and generates the translation in the target language, token by token; this architecture is called Seq2Seq model, and the way it generates the target sequence is called autoregressive model. Issues with the RNN models Difficult to retain information from a distant word That is, hard to learn long-distance dependencies “Vanishing gradients” problem Derivative of the sigmoid is always below 0.25, hence multiplying too many derivatives according to the chain rule will end up near zero values. Difficult to parallelize operations Each operation depends on the previous operation. Uni-directional Either forward or backward Attention MechanismTo improve the performance of the NMT models, attention mechanisms can be added to the decoder RNN. Instead of relying only on the final context vector, we want to “attend” to any part of the input sequence. Attention allows to focus on different parts of the input sequence, based on the relevance of each input term to the current output term.","link":"/2023/03/15/Language-Models-BERT-to-ChatGPT/"},{"title":"","text":"","link":"/2023/03/15/index/"}],"tags":[{"name":"Generative AI","slug":"Generative-AI","link":"/tags/Generative-AI/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Language Models","slug":"Language-Models","link":"/tags/Language-Models/"}],"categories":[{"name":"teaching","slug":"teaching","link":"/teaching/"}],"pages":[{"title":"about","text":"Jiho NohMy name is Jiho, and I am an assistant professor of computer science at Kennesaw State University. My primary research interests are on the fields of NLP (natural language processing) and general AI (artificial intelligence) for intelligent search. Subtopics include: neural (graph) representation learning for knowledge discovery conversational technologies for information retrieval text processing including data mining, information extraction, entity recognition, and automatic knowledge database construction neural language understanding/generation If you are interested in this research field, please contact me via email.","link":"/about/index.html"}]}