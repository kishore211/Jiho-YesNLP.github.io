{"posts":[{"title":"Language Models, BERT to ChatGPT","text":"I am hearing ‘ChatGPT’ from everywhere. Probably, you do too. I hadn’t looked into it until recently; because I thought this was another language model trained with bunch of data. (Actually, I have hundreds of paper “to read” and I didn’t want to add another to my pile.) So, eventually I played with it, and yes I was shocked by the quality of the generated responses. Here’s some examples. You can even ask it to program~! And it will return a working code even with comments. (Well, now we have GPT-4 which you can use to code a website out of a mockup image of your website.) I believe now the academia (at least) is in real danger. It seems like for now we do not have a way to tell whether a writing is created by a human or generated by a pre-trained language model (PLM). I ran a plagiarism checker and saw if this code was a copy of any piece of code accessible on the Internet. I used Turnitin and the plagiarism score was only 9% (which is much less than a homework submission of the average college students). A possible source of the code is Qiskit, which has a code base for open-source quantum development (Qiskit.optimization.applications.ising.knapsack), and I don’t think it’s likely that the code is a copy. Now, already there are a few scientific articles that added GPT as a co-author of the paper, which are accepted as legitimate papers. Again, GPT is another transformer-based language model; but, trained really well. However, there’s no way I can just close my eyes and treat it as another PLM. And, anyone who’s studying machine learning should understand how it become a thing. What is (Chat)GPT? According to Wikipedia, ChatGPT is… Generative Pretrained Transformer (GPT), Based on OpenAI’s GPT-3 family of Large Language Model (LLM), Fine-tuned using both supervised and reinforcement learning.Fine-tuned using both supervised and reinforcement learning. Alright, so let’s start with Transformer Attention Mechanism and Transformer","link":"/2023/03/15/Language-Models-BERT-to-ChatGPT/"},{"title":"","text":"","link":"/2023/03/15/index/"}],"tags":[{"name":"Generative AI","slug":"Generative-AI","link":"/tags/Generative-AI/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Language Models","slug":"Language-Models","link":"/tags/Language-Models/"}],"categories":[{"name":"teaching","slug":"teaching","link":"/teaching/"}],"pages":[{"title":"about","text":"Jiho NohMy name is Jiho, and I am an assistant professor of computer science at Kennesaw State University. My primary research interests are on the fields of NLP (natural language processing) and general AI (artificial intelligence) for intelligent search. Subtopics include: neural (graph) representation learning for knowledge discovery conversational technologies for information retrieval text processing including data mining, information extraction, entity recognition, and automatic knowledge database construction neural language understanding/generation If you are interested in this research field, please contact me via email.","link":"/about/index.html"}]}